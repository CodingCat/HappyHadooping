#!/usr/bin/env bash
# Written By Nan Zhu

# Launch an EC2 cluster of Hadoop instances.
# Inspired by Hadoop Official EC2 Tool

# Import variables
bin=`dirname "$0"`
bin=`cd "$bin"; pwd`
. "$bin"/hadoop-ec2-env.sh

if [ -z $1 ]; then
  echo "Cluster name required!"
  exit -1
fi

if [ -z $2 ]; then
  echo "Must specify the number of slaves to start."
  exit -1
fi

if ! . "$bin"/launch-hadoop-nfsserver.sh $1 $HADOOP_VERSION; then
	exit $?
fi

if ! . "$bin"/launch-hadoop-master $1 ; then
  exit $?
fi

#start rpcbind and nfs
ssh $SSH_OPTS "root@$MASTER_EC2_HOST" 'service rpcbind start'
ssh $SSH_OPTS "root@$MASTER_EC2_HOST" 'service nfs start'
ssh $SSH_OPTS "root@$MASTER_EC2_HOST" "mkdir /usr/local/hadoop-$HADOOP_VERSION"
ssh $SSH_OPTS "root@$MASTER_EC2_HOST" "mount -t nfs $NFS_PRIVATE_IP:/usr/local/hadoop-$HADOOP_VERSION /usr/local/hadoop-$HADOOP_VERSION"

# Substituting master hostname
sed -e "s|%MASTER_HOST%|$MASTER_EC2_HOST|" "$bin"/$USER_DATA_FILE > "$bin"/image/$USER_DATA_FILE.slave

if ! . "$bin"/launch-hadoop-slaves $* ; then
  exit $?
fi


#upload master file
echo "localhost" > masters
echo "Copying master file to nfs-server $NFS_HOST"
scp $SSH_OPTS masters "root@$NFS_HOST:/usr/local/hadoop-$HADOOP_VERSION/conf"


#generate slave file
OLD_IFS="$IFS"
IFS=' \n'
NUM_SERVER=`expr $2 + 2` #containing the NFS server
while true; do
	HOSTS=`ec2-describe-instances | grep INSTANCE | grep running | awk '{print \$15}'`;
	HOSTS_ADDR=($HOSTS)
	echo $HOSTS_ADDR > slaves.tmp
	NUM_RUNNING_SERVER=`cat slaves.tmp | wc -l`
	if [ $NUM_RUNNING_SERVER -eq $NUM_SERVER ]; then
		break;
	fi
	sleep 1;
done
#remove the master addr
sed -e '1d' slaves.tmp > slaves.tmp.tmp
MASTER_PRIVATE_IP=`sed -n '1,1p' slaves.tmp.tmp`
sed -e '1d' slaves.tmp.tmp > slaves
#get MASTER_PRIVATE_IP
rm slaves.tm*
#recover IFS
IFS="$OLD_IFS"

#setup slaves
echo 'setup slave nodes'
cat slaves | while read line
do
	CURRENT_SLAVE_IP=${line}
	echo "setup slave node $CURRENT_SLAVE_IP"
	#start rpcbind and nfs
	ssh $SSH_OPTS "root@$CURRENT_SLAVE_IP" 'service rpcbind start'
	ssh $SSH_OPTS "root@$CURRENT_SLAVE_IP" 'service nfs start'
	ssh $SSH_OPTS "root@$CURRENT_SLAVE_IP" "mkdir /usr/local/hadoop-$HADOOP_VERSION"
	ssh $SSH_OPTS "root@$CURRENT_SLAVE_IP" "mount -t nfs $NFS_PRIVATE_IP:/usr/local/hadoop-$HADOOP_VERSION /usr/local/hadoop-$HADOOP_VERSION"
done	

echo "Copying slave file to nfs-server $NFS_HOST"
scp $SSH_OPTS slaves "root@$NFS_HOST:/usr/local/hadoop-$HADOOP_VERSION/conf"

#upload configuration script 
echo "Copying configuring script to nfs-server $NFS_HOST"
scp $SSH_OPTS hadoop-ec2-env.sh "root@$NFS_HOST:/usr/local/hadoop-$HADOOP_VERSION/bin"
scp $SSH_OPTS "$bin"/image/$USER_DATA_FILE.slave "root@$NFS_HOST:/usr/local/hadoop-$HADOOP_VERSION/bin/$USER_DATA_FILE"
ssh $SSH_OPTS "root@$NFS_HOST" "chmod 655 /usr/local/hadoop-$HADOOP_VERSION/bin/hadoop-ec2-init-remote.sh"
ssh $SSH_OPTS "root@$MASTER_EC2_HOST" "/usr/local/hadoop-$HADOOP_VERSION/bin/hadoop-ec2-init-remote.sh"

rm "$bin"/image/$USER_DATA_FILE.slave
